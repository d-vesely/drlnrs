{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from src.rl.evaluators.evaluator_dqn import EvaluatorDQN\n",
    "from src.rl.evaluators.evaluator_c51 import EvaluatorC51\n",
    "from src.rl.evaluators.evaluator_qr import EvaluatorQR\n",
    "from src.rl.evaluators.evaluator_iqn import EvaluatorIQN\n",
    "from src.rl.evaluators.evaluator_fqf import EvaluatorFQF\n",
    "from src.rl.evaluators.evaluator_ddpg import EvaluatorDDPG\n",
    "from src.rl.evaluators.evaluator_td3 import EvaluatorTD3\n",
    "from src.rl.evaluators.evaluator_reinforce import EvaluatorREINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Baseline\n",
    "\n",
    "In the following cell, we create random predictions to be used as a lower bound baseline. We get random baselines for the average return on the development set, as well as predictions on the test set to be evaluated via the MIND competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import src.constants as constants\n",
    "from src.common_utils import read_pickled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the values, if wanted\n",
    "seeds = [7, 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = read_pickled_data([\n",
    "    constants.DEV_PATH,\n",
    "    \"preprocessed\",\n",
    "    \"behaviors.pkl\"\n",
    "])\n",
    "model_dir = os.path.join(\n",
    "    constants.MODELS_PATH,\n",
    "    \"Baseline\"\n",
    ")\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in seeds:\n",
    "    results = StringIO()\n",
    "    csv_writer = writer(results)\n",
    "    columns = [\"checkpoint\", \"mean_return\"]\n",
    "    csv_writer.writerow(columns)\n",
    "\n",
    "    pred_dir_name = \"predictions\" if seed is None else f\"predictions_{seed}\"\n",
    "    pred_dir = os.path.join(\n",
    "        model_dir,\n",
    "        pred_dir_name\n",
    "    )\n",
    "    if not os.path.exists(pred_dir):\n",
    "        os.makedirs(pred_dir)\n",
    "    #! Set seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    returns = []\n",
    "\n",
    "    for row in tqdm(eval_data.itertuples(), total=len(eval_data)):\n",
    "        shown_news = row.shown_news\n",
    "        clicked_news = set(row.clicked_news)\n",
    "\n",
    "        # Randomly order candidates\n",
    "        np.random.shuffle(shown_news)\n",
    "        \n",
    "        # Compute return\n",
    "        G = 0\n",
    "        for t, news_id in enumerate(shown_news):\n",
    "            reward = 0\n",
    "            if news_id in clicked_news:\n",
    "                reward = 1\n",
    "            G += ((gamma**t) * reward)\n",
    "        returns.append(G)\n",
    "\n",
    "    # Compute average return\n",
    "    mean_return = np.array(returns).mean()\n",
    "    csv_writer.writerow([\"Baseline\", mean_return])\n",
    "\n",
    "    print(f\"[INFO] writing evaluation results file to {pred_dir}\")\n",
    "    results.seek(0)\n",
    "    data_eval_results = pd.read_csv(results)\n",
    "    data_eval_results.to_csv(\n",
    "        os.path.join(pred_dir, \"eval_results.txt\"),\n",
    "        sep='\\t',\n",
    "        index=False,\n",
    "        header=True\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Set\n",
    "\n",
    "We compute the average return for multiple random baselines, as well as the average over all baselines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "development = True\n",
    "model_name = \"DQN-nf-trainnorm-noeng-m\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "evaluator = EvaluatorDQN(development, model_name, device, seed=7)\n",
    "evaluator.set_evaluatee()\n",
    "evaluator.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
