{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from src.rl.evaluators.evaluator_dqn import EvaluatorDQN\n",
    "from src.rl.evaluators.evaluator_c51 import EvaluatorC51\n",
    "from src.rl.evaluators.evaluator_qr import EvaluatorQR\n",
    "from src.rl.evaluators.evaluator_iqn import EvaluatorIQN\n",
    "from src.rl.evaluators.evaluator_fqf import EvaluatorFQF\n",
    "from src.rl.evaluators.evaluator_ddpg import EvaluatorDDPG\n",
    "from src.rl.evaluators.evaluator_td3 import EvaluatorTD3\n",
    "from src.rl.evaluators.evaluator_reinforce import EvaluatorREINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Baseline\n",
    "\n",
    "In the following cell, we create random predictions to be used as a lower bound baseline. We get random baselines for the average return on the development set, as well as predictions on the test set to be evaluated via the MIND competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVELOPMENT = True\n",
    "\n",
    "eval_data = read_pickled_data([\n",
    "    DEV_PATH if DEVELOPMENT else TEST_PATH,\n",
    "    \"preprocessed\",\n",
    "    \"behaviors.pkl\"\n",
    "])\n",
    "\n",
    "if DEVELOPMENT:\n",
    "    SEEDS = [42, 100, 7]\n",
    "    GAMMA = 0.9\n",
    "else:\n",
    "    SEED = 42\n",
    "    PREDICTIONS_DIR = os.path.join(\"./predictions\", \"random-baseline\", f\"{SEED}\")\n",
    "    if not os.path.exists(PREDICTIONS_DIR):\n",
    "        os.makedirs(PREDICTIONS_DIR)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare prediction buffer\n",
    "predictions = StringIO()\n",
    "csv_writer = writer(predictions)\n",
    "columns=[\"impression_id\", \"ranking\"]\n",
    "csv_writer.writerow(columns)\n",
    "\n",
    "#! Set seed\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for row in tqdm(eval_data.itertuples(), total=len(eval_data)):\n",
    "    # Get impression id and number of candidates\n",
    "    impression_id = row.id\n",
    "    num_candidates = len(row.shown_news)\n",
    "\n",
    "    # Create random ranking, add 1 (lowest rank is 1)\n",
    "    ranking = np.random.permutation(num_candidates) + 1\n",
    "\n",
    "    # Write prediction\n",
    "    pred = [impression_id, ranking]\n",
    "    csv_writer.writerow(pred)\n",
    "\n",
    "print(f\"[INFO] writing predictions file to {}\")\n",
    "predictions.seek(0)\n",
    "data_predictions = pd.read_csv(predictions)\n",
    "data_predictions[\"ranking\"] = data_predictions[\"ranking\"].progress_apply(\n",
    "    lambda x: f\"[{','.join(x[1:-1].split())}]\"\n",
    ")\n",
    "data_predictions.to_csv(\n",
    "    os.path.join(PREDICTIONS_DIR, \"prediction.txt\"),\n",
    "    sep=' ',\n",
    "    index=False,\n",
    "    header=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Set\n",
    "\n",
    "We compute the average return for multiple random baselines, as well as the average over all baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all average returns\n",
    "mean_returns = []\n",
    "std_returns = []\n",
    "\n",
    "# Evaluate multiple random baselines\n",
    "for seed in SEEDS:\n",
    "    print(f\"[INFO] evaluating random baseline, seed: {seed}\")\n",
    "    \n",
    "    #! Set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Collect returns\n",
    "    returns = []\n",
    "\n",
    "    for row in tqdm(eval_data.itertuples(), total=len(eval_data)):\n",
    "        shown_news = row.shown_news\n",
    "        clicked_news = set(row.clicked_news)\n",
    "\n",
    "        # Randomly order candidates\n",
    "        np.random.shuffle(shown_news)\n",
    "        \n",
    "        # Compute return\n",
    "        G = 0\n",
    "        for t, news_id in enumerate(shown_news):\n",
    "            reward = 0\n",
    "            if news_id in clicked_news:\n",
    "                reward = 1\n",
    "            G += ((GAMMA**t) * reward)\n",
    "        returns.append(G)\n",
    "\n",
    "    # Compute average return\n",
    "    mean_return = np.array(returns).mean()\n",
    "    std_return = np.array(returns).std()\n",
    "    print(f\"[RESULT] Return: {mean_return:.4f} +/- {std_return:.4f}\")\n",
    "\n",
    "    mean_returns.append(mean_return)\n",
    "    std_returns.append(std_return)\n",
    "\n",
    "total_mean = np.array(mean_returns).mean()\n",
    "total_mean_std = np.array(std_returns).mean()\n",
    "print(f\"\\n[RESULT] Average return over all baselines: {total_mean:.4f} +/- {total_mean_std:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] device: cuda\n",
      "[INFO] preparing predictions directory\n",
      "[INFO] reading config files\n",
      "[INFO] model checkpoints: ['actor_final.pth']\n",
      "[INFO] preparing data and sampler\n",
      "[DONE] evaluator initialized\n",
      "c:\\workbench\\developer\\drlnrs\\models\\REINFORCE-n-m\\checkpoints_7\\actor_final.pth\n",
      "[INFO] evaluating 'REINFORCE-n-m', checkpoint 'actor_final.pth'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a936215069c445749b24fb037b556465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/376471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] Return: 0.6042\n",
      "[INFO] writing evaluation results file to c:\\workbench\\developer\\drlnrs\\models\\REINFORCE-n-m\\predictions_7\n",
      "[DONE] evaluation completed\n"
     ]
    }
   ],
   "source": [
    "development = True\n",
    "model_name = \"REINFORCE-n-m\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "evaluator = EvaluatorREINFORCE(development, model_name, device, seed=7)\n",
    "evaluator.set_evaluatee()\n",
    "evaluator.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
